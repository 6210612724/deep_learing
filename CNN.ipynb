{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2eaaa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'disgust', 'fear', 'hapiness', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os,cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 10\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.utils import np_utils\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Activation , Dropout ,Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import *\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import os\n",
    "print(os.listdir(r\"C:\\Users\\thanawin\\Desktop\\Deep-learing\\90_10_10\\detect_split\\train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b0f8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.datasets import make_classification\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11c0393c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the images of dataset-anger\n",
      "\n",
      "Loaded the images of dataset-disgust\n",
      "\n",
      "Loaded the images of dataset-fear\n",
      "\n",
      "Loaded the images of dataset-hapiness\n",
      "\n",
      "Loaded the images of dataset-neutral\n",
      "\n",
      "Loaded the images of dataset-sad\n",
      "\n",
      "Loaded the images of dataset-surprise\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20451, 128, 128, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = r'C:\\Users\\thanawin\\Desktop\\Deep-learing\\90_10_10\\detect_split\\train'\n",
    "data_dir_list = os.listdir(data_path)\n",
    "\n",
    "num_epoch=10\n",
    "\n",
    "img_data_list=[]\n",
    "\n",
    "\n",
    "for dataset in data_dir_list:\n",
    "    img_list=os.listdir(data_path+'/'+ dataset)\n",
    "    print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "    for img in img_list:\n",
    "        input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
    "        #input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "        input_img_resize=cv2.resize(input_img,(128,128))\n",
    "        img_data_list.append(input_img_resize)\n",
    "        \n",
    "img_data = np.array(img_data_list)\n",
    "img_data = img_data.astype('float32')\n",
    "img_data = img_data/255\n",
    "img_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1785c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "\n",
    "num_of_samples = img_data.shape[0]\n",
    "labels = np.ones((num_of_samples,),dtype='int64')\n",
    "\n",
    "labels[0:3896]=0 #3897 0 - 3896\n",
    "labels[3897:6435]=1 #2538 6435\n",
    "labels[6436:8752]=2 #2317 8752\n",
    "labels[8753:11830]=3 #3078 11830\n",
    "labels[11831:14307]=4 #2477 14307\n",
    "labels[14308:16991]=5 #2684 16991\n",
    "labels[16992:20451]=6 #3460 20451\n",
    "\n",
    "names = ['\n",
    "$ ls\n",
    "0/  1/  2/  3/  4/  5/  6/\n",
    "\n",
    "','disgust','fear','hapiness','neutral','sad','surprise']\n",
    "\n",
    "def getLabel(id):\n",
    "    return ['anger','disgust','fear','hapiness','neutral','sad','surprise'][id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a715a84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np_utils.to_categorical(labels, num_classes)\n",
    "\n",
    "#Shuffle the dataset\n",
    "x,y = shuffle(img_data,Y, random_state=2)\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=2)\n",
    "x_test=X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "981a40f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "def create_model():\n",
    "        model = Sequential() #Sequential model allows to build a model layer by layer\n",
    "\n",
    "        #--------------------------------Input layer------------------------------------------# \n",
    "        #COnvolution layer parametrs:\n",
    "        #No of output filters: 32\n",
    "        #kernel_size: specifying the height and width of the 2D convolution window : (3,3)\n",
    "        #activation function : relu\n",
    "        #input_shape : shape of image matrix : 48x48x1 : (Gray Scale Image)\n",
    "        model.add(Conv2D(8, kernel_size=(3, 3), kernel_initializer='he_uniform', activation='elu', input_shape=(128,128,3))) \n",
    "        #Batch Normalization : It is used to normalize the output of the previous layers. The activations scale the input layer in normalization.\n",
    "        #Using batch normalization learning becomes efficient also it can be used as regularization to avoid overfitting of the model.  \n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        #--------------------------------Second layer------------------------------------------# \n",
    "        model.add(Conv2D(8, kernel_size=(3, 3), kernel_initializer='he_uniform', activation='elu')) \n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2))) #Down-sampling opertaion to reduce dimensionality of feature map\n",
    "        model.add(Dropout(0.2)) #regularization technique to reduce overfitting: dropouts randomly switches off some neurons in the network which forces the data to find new paths\n",
    "\n",
    "        #--------------------------------Third layer------------------------------------------# \n",
    "        model.add(Conv2D(16, kernel_size=(3, 3), kernel_initializer='he_uniform', activation='elu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        #--------------------------------Third layer------------------------------------------# \n",
    "        model.add(Conv2D(32, kernel_size=(3, 3), kernel_initializer='he_uniform', activation='elu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        #--------------------------------Fourth layer------------------------------------------# \n",
    "        model.add(Conv2D(64, kernel_size=(3, 3), kernel_initializer='he_uniform', activation='elu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        #--------------------------------five layer------------------------------------------# \n",
    "        model.add(Conv2D(128, kernel_size=(3, 3), kernel_initializer='he_uniform', activation='elu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        #--------------------------------six layer------------------------------------------# \n",
    "        model.add(Conv2D(256, kernel_size=(3, 3), kernel_initializer='he_uniform', activation='elu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        #--------------------------------six layer------------------------------------------# \n",
    "        model.add(Conv2D(512, kernel_size=(3, 3), kernel_initializer='he_uniform', activation='elu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        #--------------------------------six layer------------------------------------------# \n",
    "        model.add(Conv2D(1024, kernel_size=(3, 3), kernel_initializer='he_uniform', activation='elu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "        model.add(Flatten()) #converts 3d vector to 1 d vector : Flatten serves as a connection between the convolution and dense layers.\n",
    "\n",
    "        #--------------------------------Output layers------------------------------------------# \n",
    "        model.add(Dense(512, kernel_initializer='he_uniform', activation='elu')) #Dense layer predict labels\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(512, kernel_initializer='he_uniform', activation='elu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(7, activation='softmax'))        #FINAL PREDICTION\n",
    "        #Softmax makes the output sum up to 1 so the output can be interpreted as probabilities.\n",
    "        model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='RMSprop')\n",
    "    \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "467ad183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=25, width_shift_range=0.1,\n",
    "    height_shift_range=0.1, shear_range=0.2, \n",
    "    zoom_range=0.2,horizontal_flip=True, \n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e621228",
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 32\n",
    "EPOCHS = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83212262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_13848\\1376269716.py:18: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  hist = model.fit_generator(aug.flow(X_Train_, Y_Train), epochs=EPOCHS,validation_data=(X_Test_, Y_Test), callbacks=callbacks_list, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "scores_loss = []\n",
    "scores_acc = []\n",
    "k_no = 0\n",
    "for train_index, test_index in kf.split(x):\n",
    "    X_Train_ = x[train_index]\n",
    "    Y_Train = y[train_index]\n",
    "    X_Test_ = x[test_index]\n",
    "    Y_Test = y[test_index]\n",
    "\n",
    "    file_path = r\"C:\\Users\\thanawin\\Desktop\\Deep-learing\\90_10_10\\weight\\weights_best_\"+str(k_no)+\".hdf5\"\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "    early = EarlyStopping(monitor=\"loss\", mode=\"min\", patience=8)\n",
    "\n",
    "    callbacks_list = [checkpoint, early]\n",
    "\n",
    "    model = create_model()\n",
    "    hist = model.fit_generator(aug.flow(X_Train_, Y_Train), epochs=EPOCHS,validation_data=(X_Test_, Y_Test), callbacks=callbacks_list, verbose=0)\n",
    "    # model.fit(X_Train, Y_Train, batch_size=batch_size, epochs=epochs, validation_data=(X_Test, Y_Test), verbose=1)\n",
    "    model.load_weights(file_path)\n",
    "    result.append(model.predict(X_Test_))\n",
    "    score = model.evaluate(X_Test_,Y_Test, verbose=0)\n",
    "    scores_loss.append(score[0])\n",
    "    scores_acc.append(score[1])\n",
    "    k_no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6bda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores_acc,scores_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f13e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_min = min(scores_loss)\n",
    "value_index = scores_loss.index(value_min)\n",
    "print(value_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb53035",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(r\"C:\\Users\\thanawin\\Desktop\\Deep-learing\\90_10_10\\weight\\weights_best_\"+str(value_index)+\".hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4e42b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0bee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "test_image = X_test[0:1]\n",
    "print (test_image.shape)\n",
    "\n",
    "print(best_model.predict(test_image))\n",
    "print(best_model.predict_classes(test_image))\n",
    "print(y_test[0:1])\n",
    "\n",
    "#predict\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6472f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing losses and accuracy\n",
    "%matplotlib inline\n",
    "\n",
    "train_loss=hist.history['loss']\n",
    "val_loss=hist.history['val_loss']\n",
    "train_acc=hist.history['accuracy']\n",
    "val_acc=hist.history['val_accuracy']\n",
    "\n",
    "epochs = range(len(train_acc))\n",
    "\n",
    "plt.plot(epochs,train_loss,'r', label='train_loss')\n",
    "plt.plot(epochs,val_loss,'b', label='val_loss')\n",
    "plt.title('train_loss vs val_loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,train_acc,'r', label='train_acc')\n",
    "plt.plot(epochs,val_acc,'b', label='val_acc')\n",
    "plt.title('train_acc vs val_acc')\n",
    "plt.legend()\n",
    "plt.figure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
